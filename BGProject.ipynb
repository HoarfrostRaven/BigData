{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoarfrostRaven/BigData/blob/main/BGProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare"
      ],
      "metadata": {
        "id": "W-slv4jGsigh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://snap.stanford.edu/data/web-Google.txt.gz\n",
        "!gzip -d web-Google.txt.gz"
      ],
      "metadata": {
        "id": "vBrWu6gUqQxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7e808a-ff7d-4273-f6fe-668396d2f68c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-29 13:01:08--  https://snap.stanford.edu/data/web-Google.txt.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21168784 (20M) [application/x-gzip]\n",
            "Saving to: ‘web-Google.txt.gz’\n",
            "\n",
            "web-Google.txt.gz   100%[===================>]  20.19M  5.72MB/s    in 3.5s    \n",
            "\n",
            "2025-03-29 13:01:12 (5.72 MB/s) - ‘web-Google.txt.gz’ saved [21168784/21168784]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCXIZ8LXp8zD",
        "outputId": "b94c25eb-4447-4ba2-b1f4-f37fd7699b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVFcipjRp-Pw",
        "outputId": "2620b15d-43b5-47ff-b680-cb3fd660f538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  web-Google.txt  web-Google.txt.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EaKhS0f_kRE9"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark\n",
        "import random\n",
        "from pyspark import SparkConf\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.context import SparkContext\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CCF"
      ],
      "metadata": {
        "id": "q4B-gy9ZuS67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def connected_components_ccf(data, num_partitions = 1):\n",
        "    \"\"\"\n",
        "    Computes connected components using the CCF algorithm.\n",
        "\n",
        "    Args:\n",
        "        data: An RDD of edges represented as tuples (node1, node2).\n",
        "              Example: sc.parallelize([(1, 2), (2, 3), (2, 4), (4, 5), (6, 7), (7, 8)])\n",
        "        num_partitions: Number of partitions to optimize parallelism.\n",
        "\n",
        "    Returns:\n",
        "        An RDD of edges representing the connected components.\n",
        "        Example: sc.parallelize([(2, 1), (3, 1), (4, 1), (5, 1), (7, 6), (8, 6)])\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize bidirectional edges with partition optimization\n",
        "    edges = (data.flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "             .distinct()\n",
        "             .repartition(num_partitions)\n",
        "             .persist(StorageLevel.MEMORY_AND_DISK))  # Spill to disk if OOM\n",
        "\n",
        "    converged = False\n",
        "    iteration = 0\n",
        "    prev_edges = None  # Track previous iteration's data\n",
        "\n",
        "    while not converged:\n",
        "        iteration += 1\n",
        "        print(f\"--- Iteration {iteration} ---\")\n",
        "\n",
        "        # Filter first to reduce data volume before reduce\n",
        "        filtered = edges.filter(lambda x: x[1] < x[0]).cache()\n",
        "\n",
        "        # Compute minimum neighbors with partition preservation\n",
        "        min_values = filtered.reduceByKey(min, numPartitions=num_partitions).cache()\n",
        "        filtered.unpersist()  # Release intermediate data immediately\n",
        "\n",
        "        # Join with partition alignment to minimize shuffle\n",
        "        new_edges = (min_values.join(edges, numPartitions=num_partitions)\n",
        "                     .filter(lambda x: x[1][0] != x[1][1])  # Remove self-edges\n",
        "                     .map(lambda x: (x[1][1], x[1][0]))     # Remap edges\n",
        "                     .cache())\n",
        "\n",
        "        if new_edges.isEmpty():\n",
        "            converged = True\n",
        "            result = min_values\n",
        "        else:\n",
        "            # Release data from two iterations back\n",
        "            if prev_edges is not None:\n",
        "                prev_edges.unpersist()\n",
        "\n",
        "            # Track current edges for future cleanup\n",
        "            prev_edges = edges\n",
        "\n",
        "            # Update edges with partition optimization\n",
        "            edges = (min_values.union(new_edges)\n",
        "                     .flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "                     .distinct()\n",
        "                     .repartition(num_partitions)\n",
        "                     .persist(StorageLevel.MEMORY_AND_DISK))\n",
        "\n",
        "            # Release current iteration's intermediates\n",
        "            min_values.unpersist()\n",
        "            new_edges.unpersist()\n",
        "\n",
        "    # Final cleanup\n",
        "    edges.unpersist()\n",
        "    return result"
      ],
      "metadata": {
        "id": "YVj6TpKoEmiZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CCF Sorting"
      ],
      "metadata": {
        "id": "b-frwmNKO3ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def connected_components_ccf_sorting(data, num_partitions=1):\n",
        "    \"\"\"\n",
        "    Computes connected components using the CCF algorithm (with sorting instead of reduceByKey).\n",
        "\n",
        "    Args:\n",
        "        data: An RDD of edges represented as tuples (node1, node2).\n",
        "        num_partitions: Number of partitions to optimize parallelism.\n",
        "\n",
        "    Returns:\n",
        "        An RDD of edges representing the connected components.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize bidirectional edges with partition optimization\n",
        "    edges = (data.flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "             .distinct()\n",
        "             .repartition(num_partitions)\n",
        "             .persist(StorageLevel.MEMORY_AND_DISK))  # Spill to disk if OOM\n",
        "\n",
        "    converged = False\n",
        "    iteration = 0\n",
        "    prev_edges = None  # Track previous iteration's data\n",
        "\n",
        "    while not converged:\n",
        "        iteration += 1\n",
        "        print(f\"--- Iteration {iteration} ---\")\n",
        "\n",
        "        # Filter to keep (higher_node, lower_node) pairs\n",
        "        filtered = edges.filter(lambda x: x[1] < x[0]).cache()\n",
        "\n",
        "        # Compute minimum neighbor using sorting\n",
        "        min_values = (filtered.groupByKey(num_partitions)\n",
        "                      .mapValues(lambda neighbors: sorted(neighbors)[0])  # Sort and take min\n",
        "                      .cache())\n",
        "\n",
        "        filtered.unpersist()  # Release intermediate data immediately\n",
        "\n",
        "        # Join with partition alignment to minimize shuffle\n",
        "        new_edges = (min_values.join(edges, num_partitions)\n",
        "                     .filter(lambda x: x[1][0] != x[1][1])  # Remove self-edges\n",
        "                     .map(lambda x: (x[1][1], x[1][0]))     # Remap edges\n",
        "                     .cache())\n",
        "\n",
        "        if new_edges.isEmpty():\n",
        "            converged = True\n",
        "            result = min_values\n",
        "        else:\n",
        "            # Release data from two iterations back\n",
        "            if prev_edges is not None:\n",
        "                prev_edges.unpersist()\n",
        "\n",
        "            # Track current edges for future cleanup\n",
        "            prev_edges = edges\n",
        "\n",
        "            # Update edges with partition optimization\n",
        "            edges = (min_values.union(new_edges)\n",
        "                     .flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "                     .distinct()\n",
        "                     .repartition(num_partitions)\n",
        "                     .persist(StorageLevel.MEMORY_AND_DISK))\n",
        "\n",
        "            # Release current iteration's intermediates\n",
        "            min_values.unpersist()\n",
        "            new_edges.unpersist()\n",
        "\n",
        "    # Final cleanup\n",
        "    edges.unpersist()\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "JzKT1K8aO5f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 计算 num_partitions"
      ],
      "metadata": {
        "id": "ObcqYxCaL6AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_rdd_size_gb(rdd, sample_fraction=0.01):\n",
        "    \"\"\"\n",
        "    估算 RDD 数据的大小（GB），通过采样计算平均行大小并推测整体数据大小。\n",
        "\n",
        "    :param rdd: 要估算的 Spark RDD\n",
        "    :param sample_fraction: 采样比例（默认 1%），用于计算平均行大小\n",
        "    :return: 估算的 RDD 大小（GB）\n",
        "    \"\"\"\n",
        "    from pyspark import StorageLevel\n",
        "\n",
        "    # 采样 RDD（非放回抽样）\n",
        "    sample_data = rdd.sample(False, sample_fraction)\n",
        "\n",
        "    # 计算平均每行字节大小（转换为字符串后编码为 UTF-8 字节）\n",
        "    avg_line_size = sample_data.map(lambda x: len(str(x).encode('utf-8'))).mean()\n",
        "\n",
        "    # 计算 RDD 总行数\n",
        "    num_lines = rdd.count()\n",
        "\n",
        "    # 估算 RDD 总大小（字节）\n",
        "    data_size_bytes = num_lines * avg_line_size\n",
        "\n",
        "    # 转换为 GB\n",
        "    data_size_gb = data_size_bytes / (1024 ** 3)\n",
        "\n",
        "    print(f\"Estimated RDD Size: {data_size_gb:.4f} GB\")\n",
        "    return data_size_gb"
      ],
      "metadata": {
        "id": "3KkmNtiF-xcg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimal_num_partitions(sc, data_size_gb=None):\n",
        "    \"\"\"\n",
        "    计算 Spark 任务的最佳分区数\n",
        "    :param sc: SparkContext\n",
        "    :param data_size_gb: 数据大小 (GB)，可选\n",
        "    :return: 计算得到的最佳分区数\n",
        "    \"\"\"\n",
        "    # 获取可用 CPU 核心数\n",
        "    available_cores = sc.defaultParallelism  # 等价于 os.cpu_count() 或 psutil.cpu_count(logical=True)\n",
        "\n",
        "    # 基于 CPU 核心数的推荐分区数\n",
        "    cpu_based_partitions = 2 * available_cores\n",
        "\n",
        "    # 基于数据量的分区数（假设每个分区 128MB）\n",
        "    if data_size_gb:\n",
        "        partition_size_mb = 128  # 假设单个分区最佳大小 128MB\n",
        "        data_size_mb = data_size_gb * 1024\n",
        "        data_based_partitions = max(1, int(data_size_mb / partition_size_mb))\n",
        "    else:\n",
        "        data_based_partitions = cpu_based_partitions  # 无数据大小时默认使用 CPU 计算\n",
        "\n",
        "    # 取两者中的较大值，确保计算不会受限\n",
        "    optimal_partitions = max(cpu_based_partitions, data_based_partitions)\n",
        "\n",
        "    print(f\"Optimal Partitions: {optimal_partitions} (CPU-based: {cpu_based_partitions}, Data-based: {data_based_partitions})\")\n",
        "    return optimal_partitions"
      ],
      "metadata": {
        "id": "vx795Lad-Ca1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 代码测试"
      ],
      "metadata": {
        "id": "tdBdgMJoL-r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data for testing\n",
        "test_data = sc.parallelize([(1, 2), (2, 3), (2, 4), (4, 5), (6, 7), (7, 8)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLHVN6RN-6dd",
        "outputId": "1d2d4d78-b7fd-4868-9349-173446abf7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iteration 1 ---\n",
            "--- Iteration 2 ---\n",
            "--- Iteration 3 ---\n",
            "--- Iteration 4 ---\n",
            "Connected components: [(4, 1), (2, 1), (3, 1), (5, 1), (7, 6), (8, 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf algorithm\n",
        "result = connected_components_ccf(test_data)\n",
        "\n",
        "# Collect and print the result\n",
        "print(\"Connected components:\", result.collect())"
      ],
      "metadata": {
        "id": "HDSBFOqgRw0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf_sorting algorithm\n",
        "result = connected_components_ccf_sorting(test_data)\n",
        "\n",
        "# Collect and print the result\n",
        "print(\"Connected components:\", result.collect())"
      ],
      "metadata": {
        "id": "-_GfW1ViRywL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 大数据实验"
      ],
      "metadata": {
        "id": "mbolMfW4JAjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file, ignore first 4 lines and split the data\n",
        "data = (\n",
        "    sc.textFile(\"/content/web-Google.txt\")\n",
        "    .filter(lambda line: line.strip() and not line.startswith('#'))\n",
        "    .map(lambda line: tuple(map(int, line.split())))\n",
        ")"
      ],
      "metadata": {
        "id": "XWM3KNZOqgtm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.take(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blZas8QJsP7l",
        "outputId": "92a40741-8510-41c2-b192-c3abd5d234e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 11342), (0, 824020), (0, 867923), (0, 891835), (11342, 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_size_gb = estimate_rdd_size_gb(data, sample_fraction=0.01)\n",
        "num_partitions = get_optimal_num_partitions(sc, data_size_gb)\n",
        "\n",
        "print(f\"Estimated RDD Size: {data_size_gb:.4f} GB\")\n",
        "print(f\"Optimal Partitions: {num_partitions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyclWR_C_AUz",
        "outputId": "30a933e3-9043-4d17-9fb7-979be839c109"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated RDD Size: 0.0750 GB\n",
            "Optimal Partitions: 4 (CPU-based: 4, Data-based: 1)\n",
            "Estimated RDD Size: 0.0750 GB\n",
            "Optimal Partitions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute connected components using the CCF algorithm\n",
        "connected_components = connected_components_ccf(data, num_partitions)"
      ],
      "metadata": {
        "id": "iktS6jBly6vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print(connected_components.collect())"
      ],
      "metadata": {
        "id": "oNW7giSB_CLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute connected components using the CCF-Sorting algorithm\n",
        "connected_components_sorting = connected_components_ccf_sorting(data, num_partitions)"
      ],
      "metadata": {
        "id": "5H9EH5iaSNVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print(connected_components_sorting.collect())"
      ],
      "metadata": {
        "id": "wjl8E9KUSSMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 随即图实验"
      ],
      "metadata": {
        "id": "UtXekNeeJb1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_graph(num_nodes, num_edges, seed=None):\n",
        "    \"\"\"\n",
        "    生成一个随机无向图，返回边列表，格式为 (int, int)。\n",
        "\n",
        "    :param num_nodes: 节点数量\n",
        "    :param num_edges: 边的数量\n",
        "    :param seed: 随机种子（可选，保证可复现）\n",
        "    :return: 由 (node1, node2) 组成的边列表\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    edges = set()\n",
        "\n",
        "    while len(edges) < num_edges:\n",
        "        node1 = random.randint(0, num_nodes - 1)\n",
        "        node2 = random.randint(0, num_nodes - 1)\n",
        "\n",
        "        # 确保无自环、无重复边（无向图存储较小节点在前）\n",
        "        if node1 != node2:\n",
        "            edge = (min(node1, node2), max(node1, node2))\n",
        "            edges.add(edge)\n",
        "\n",
        "    return list(edges)"
      ],
      "metadata": {
        "id": "eGN2z0y4Jejf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test1"
      ],
      "metadata": {
        "id": "Wb3ez21oS8DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for testing 1\n",
        "random_graph_1 = generate_random_graph(10000, 50000)\n",
        "random_graph_1_rdd = sc.parallelize(random_graph_1)"
      ],
      "metadata": {
        "id": "oxQeHv6IKdm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_size_gb = estimate_rdd_size_gb(random_graph_1_rdd, sample_fraction=0.01)\n",
        "num_partitions = get_optimal_num_partitions(sc, data_size_gb)\n",
        "\n",
        "print(f\"Estimated RDD Size: {data_size_gb:.4f} GB\")\n",
        "print(f\"Optimal Partitions: {num_partitions}\")"
      ],
      "metadata": {
        "id": "iAed4cjnUnNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf algorithm\n",
        "random_graph_1_result = connected_components_ccf(random_graph_1_rdd, num_partitions)"
      ],
      "metadata": {
        "id": "b3dAFMC_SuTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and print the result\n",
        "print(\"Connected components:\", random_graph_1_result.collect())"
      ],
      "metadata": {
        "id": "Tg8AuSahKqTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf algorithm\n",
        "random_graph_1_result_sorting = connected_components_ccf_sorting(random_graph_1_rdd, num_partitions)"
      ],
      "metadata": {
        "id": "u94NuQt-SyPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and print the result\n",
        "print(\"Connected components:\", random_graph_1_result_sorting.collect())"
      ],
      "metadata": {
        "id": "C4WvegEjSx-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 2"
      ],
      "metadata": {
        "id": "b799a4bRTDFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for testing 2\n",
        "random_graph_2 = generate_random_graph(100000, 1000000)\n",
        "random_graph_2_rdd = sc.parallelize(random_graph_2)"
      ],
      "metadata": {
        "id": "zR1WwtbQKsCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_size_gb = estimate_rdd_size_gb(random_graph_2_rdd, sample_fraction=0.01)\n",
        "num_partitions = get_optimal_num_partitions(sc, data_size_gb)\n",
        "\n",
        "print(f\"Estimated RDD Size: {data_size_gb:.4f} GB\")\n",
        "print(f\"Optimal Partitions: {num_partitions}\")"
      ],
      "metadata": {
        "id": "U6nOraQ8U2ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf algorithm\n",
        "random_graph_2_result = connected_components_ccf(random_graph_2_rdd, num_partitions)"
      ],
      "metadata": {
        "id": "zRyk5eJQSwED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and print the result\n",
        "print(\"Connected components:\", random_graph_2_result.collect())"
      ],
      "metadata": {
        "id": "bpmTVycsK1es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf algorithm\n",
        "random_graph_2_result_sorting = connected_components_ccf_sorting(random_graph_2_rdd, num_partitions)"
      ],
      "metadata": {
        "id": "0idYHOHeS2Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and print the result\n",
        "print(\"Connected components:\", random_graph_2_result_sorting.collect())"
      ],
      "metadata": {
        "id": "M9rzJKNqS1vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 3"
      ],
      "metadata": {
        "id": "pb1uiuiQuJRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for testing 3\n",
        "random_graph_3 = generate_random_graph(10000000, 50000000)\n",
        "random_graph_3_rdd = sc.parallelize(random_graph_3)"
      ],
      "metadata": {
        "id": "Zo8VxdpPuyQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_size_gb = estimate_rdd_size_gb(random_graph_3_rdd, sample_fraction=0.01)\n",
        "num_partitions = get_optimal_num_partitions(sc, data_size_gb)\n",
        "\n",
        "print(f\"Estimated RDD Size: {data_size_gb:.4f} GB\")\n",
        "print(f\"Optimal Partitions: {num_partitions}\")"
      ],
      "metadata": {
        "id": "1iAy7oZpu1Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf algorithm\n",
        "random_graph_3_result = connected_components_ccf(random_graph_3_rdd, num_partitions)"
      ],
      "metadata": {
        "id": "5uOsfSysu2sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and print the result\n",
        "print(\"Connected components:\", random_graph_3_result.collect())"
      ],
      "metadata": {
        "id": "zE9ZdQX2u66i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the connected_components_ccf algorithm\n",
        "random_graph_3_result_sorting = connected_components_ccf_sorting(random_graph_3_rdd, num_partitions)"
      ],
      "metadata": {
        "id": "c_6oWKJGu8_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and print the result\n",
        "print(\"Connected components:\", random_graph_3_result_sorting.collect())"
      ],
      "metadata": {
        "id": "C7s1hYEau_M3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}